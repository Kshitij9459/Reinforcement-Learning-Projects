{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs=env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render(mode='rgb_array')  \n",
    "n_outputs=env.action_space.n\n",
    "n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "  \n",
    "    img=img[:180,:,:]/255\n",
    "    img=cv2.resize(img,(88,88))\n",
    "    X=np.zeros((88,88,1))\n",
    "    X[:,:,0]=np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def QNetwork(X,scope):\n",
    "  \n",
    "  #Making structure of Q_Network and collecting weights\n",
    "    \n",
    "    initializer=tf.contrib.layers.variance_scaling_initializer()\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "    \n",
    "        conv1=tf.layers.conv2d(inputs=X,kernel_size=(8,8),strides=(4,4),filters=32,padding='SAME',kernel_initializer=initializer)\n",
    "    \n",
    "        conv2=tf.layers.conv2d(inputs=conv1,filters=64,kernel_size=(4,4),strides=(2,2),padding='SAME',kernel_initializer=initializer)\n",
    "    \n",
    "        conv3=tf.layers.conv2d(inputs=conv2,filters=64,kernel_size=(3,3),strides=(1,1),padding='SAME',kernel_initializer=initializer)\n",
    "    \n",
    "        flat=tf.layers.flatten(conv3)\n",
    "    \n",
    "        fc = tf.contrib.layers.fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "    \n",
    "        output=tf.contrib.layers.fully_connected(fc, num_outputs=n_outputs,activation_fn=None,weights_initializer=initializer)\n",
    "    \n",
    "        var = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "    \n",
    "        return var,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.5\n",
    "eps_min=0.05\n",
    "eps_max=1\n",
    "K=0.00001\n",
    "num_episodes=1000\n",
    "steps_per_action={}\n",
    "global_step=0\n",
    "train_step=5\n",
    "start_step=200\n",
    "batch_size=32\n",
    "discount_factor=0.8\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(action,steps):\n",
    "  \n",
    "    epsilon=max(eps_min,eps_max-K*steps)\n",
    "  \n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "  \n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(frame,action,reward,next_frame,done):\n",
    "    \n",
    "    if(len(replay_memory)==memory_capacity):\n",
    "        replay_memory.popleft()\n",
    "      \n",
    "    replay_memory.append([frame,action,reward,next_frame,done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_capacity=1000000\n",
    "replay_memory=deque([])       #Holding frame set\n",
    "\n",
    "def sample(batch_size):\n",
    "  \n",
    "    perm=np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    store=np.array(replay_memory)[perm]\n",
    "    \n",
    "    return store[:,0],store[:,1],store[:,2],store[:,3],store[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape=(88,88)\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=(None,img_shape[0],img_shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
    "mainQ, mainQ_outputs = QNetwork(X, 'mainQ')\n",
    "# similarly we build our target Q network\n",
    "targetQ, targetQ_outputs = QNetwork(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores action array batch-wise\n",
    "X_action=tf.placeholder(dtype=tf.int32,shape=(None,))\n",
    "#computes Q(s,a) value for training\n",
    "Q_action=tf.reduce_sum(mainQ_outputs*tf.one_hot(X_action,n_outputs),axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy weights from mainQ to targetQ\n",
    "copy_op = [tf.assign(target_name, mainQ[var_name]) for var_name, target_name in targetQ.items()]\n",
    "copy_weights = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/garg/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "y=tf.placeholder(tf.float32,shape=(None,1))\n",
    "cost=tf.reduce_mean(tf.square(y-Q_action))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e18e03940d4ad088b08d90e269c011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1245 reward 670.0\n",
      "epoch 599 reward 200.0\n",
      "epoch 693 reward 220.0\n",
      "epoch 660 reward 140.0\n",
      "epoch 753 reward 150.0\n",
      "epoch 540 reward 180.0\n",
      "epoch 631 reward 260.0\n",
      "epoch 512 reward 100.0\n",
      "epoch 713 reward 230.0\n",
      "epoch 865 reward 460.0\n",
      "epoch 717 reward 240.0\n",
      "epoch 751 reward 190.0\n",
      "epoch 622 reward 210.0\n",
      "epoch 621 reward 210.0\n",
      "epoch 536 reward 130.0\n",
      "epoch 646 reward 200.0\n",
      "epoch 672 reward 240.0\n",
      "epoch 758 reward 360.0\n",
      "epoch 762 reward 260.0\n",
      "epoch 618 reward 240.0\n",
      "epoch 768 reward 250.0\n",
      "epoch 829 reward 1740.0\n",
      "epoch 533 reward 210.0\n",
      "epoch 626 reward 250.0\n",
      "epoch 603 reward 250.0\n",
      "epoch 482 reward 150.0\n",
      "epoch 717 reward 300.0\n",
      "epoch 1100 reward 510.0\n",
      "epoch 655 reward 220.0\n",
      "epoch 520 reward 120.0\n",
      "epoch 695 reward 290.0\n",
      "epoch 691 reward 270.0\n",
      "epoch 711 reward 310.0\n",
      "epoch 531 reward 190.0\n",
      "epoch 628 reward 150.0\n",
      "epoch 482 reward 110.0\n",
      "epoch 522 reward 150.0\n",
      "epoch 715 reward 240.0\n",
      "epoch 531 reward 190.0\n",
      "epoch 602 reward 190.0\n",
      "epoch 1003 reward 370.0\n",
      "epoch 715 reward 330.0\n",
      "epoch 638 reward 370.0\n",
      "epoch 615 reward 250.0\n",
      "epoch 505 reward 120.0\n",
      "epoch 672 reward 240.0\n",
      "epoch 621 reward 180.0\n",
      "epoch 528 reward 160.0\n",
      "epoch 480 reward 180.0\n",
      "epoch 729 reward 380.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  \n",
    "    init.run()\n",
    "  \n",
    "    #for each episode i\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "    \n",
    "        obs=env.reset()\n",
    "        done=False\n",
    "        reward_per_episode=0\n",
    "        epoch=0\n",
    "     \n",
    "        while not done:\n",
    "            #Reduce image size and convert it to black and white\n",
    "            obs=preprocess_img(obs)\n",
    "      \n",
    "            actions=mainQ_outputs.eval(feed_dict={X:[obs]})\n",
    "      \n",
    "            action=np.argmax(actions,axis=-1)\n",
    "        \n",
    "      \n",
    "            action=best_action(action,global_step)\n",
    "      \n",
    "           # steps_per_action[action]=steps_per_action[action]+1\n",
    "            #print(action)\n",
    "            next_obs,reward,done,_= env.step(action)\n",
    "      \n",
    "            next_obs=preprocess_img(next_obs)\n",
    "      \n",
    "            add(obs,action,reward,next_obs,done)\n",
    "      \n",
    "            if global_step%train_step==0 and global_step>start_step:\n",
    "        \n",
    "                obs_arr,action_arr,reward_arr,next_obs_arr,done_arr=sample(batch_size)\n",
    "        \n",
    "                obs_arr=[x for x in obs_arr]\n",
    "        \n",
    "                next_obs_arr=[x for x in next_obs_arr]\n",
    "        \n",
    "                y_o=reward_arr+discount_factor*np.max(targetQ_outputs.eval(feed_dict={X:next_obs_arr}))*(1-done_arr)\n",
    "                        \n",
    "                \n",
    "                train_loss,opt=sess.run([cost,optimizer],feed_dict={X:obs_arr,y:np.expand_dims(y_o,axis=-1),X_action:action_arr})\n",
    "                \n",
    "            if global_step%50==0 and global_step>start_step:\n",
    "                \n",
    "                copy_weights.run()\n",
    "                \n",
    "            global_step+=1\n",
    "            reward_per_episode+=reward\n",
    "            epoch+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "        print('epoch',epoch,'reward',reward_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 6, 7, array([0]), 3, 0, array([1]), array([0])], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
